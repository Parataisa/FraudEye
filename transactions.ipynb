{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44270e5d",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Financial-Transactions\" data-toc-modified-id=\"Financial-Transactions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Financial Transactions</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Leaderboard-Predict-function\" data-toc-modified-id=\"The-Leaderboard-Predict-function-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>The Leaderboard Predict function</a></span></li><li><span><a href=\"#Testing-your-Implementation\" data-toc-modified-id=\"Testing-your-Implementation-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Testing your Implementation</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ab8fb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64f7c372b96e90860c81abab27fcaf91",
     "grade": false,
     "grade_id": "cell-e73d576a3d47c12f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Financial Transactions\n",
    "\n",
    "The ability to identify fraudulent transactions is of great interest to the payments industry. In this notebook, you will make use of the binary classifier you trained on the transcations dataset to detect fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1da42cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pathlib\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cbca8b0c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f55791312e024fe367642edd8b05b8c9",
     "grade": false,
     "grade_id": "cell-b9b680c4d8d3fe22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "path = \"/data/mlproject22\" if os.path.exists(\"/data/mlproject22\") else \".\"\n",
    "train_data = pd.read_csv(os.path.join(path, \"transactions.csv.zip\"))\n",
    "X_train = train_data.drop(columns = \"Class\")\n",
    "y_train = train_data[\"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61537e9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fcfbf4adef8ec43129f89597b7c59497",
     "grade": false,
     "grade_id": "cell-039042e3f5c8bdaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Leaderboard Predict function\n",
    "Replace the comment and `NotImplementedError` in the `leader_board_predict_fn` with code that loads your model parameters and returns the likelyhood of fraud for each transaction (i.e. row) in the values dataframe. Note that the returned array should contain a single decision function value for each transaction, indicating whether the transaction is fraudulent (i.e. it belongs to target class $1$). The higher the decision function value, the more likely that the transaction is fraud.\n",
    "You can import the packages you require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc399cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0fc081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "MOMENTUM = 0.9\n",
    "NUM_EPOCHS = 1\n",
    "HIDDEN_SIZE = 100\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 64\n",
    "INPUT_SIZE = 784  \n",
    "OUTPUT_SIZE = 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "858d3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04b8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e5ba22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Convert your DataFrame to tensors\n",
    "train_data = TensorDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "test_data = TensorDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "# Create your DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da85df4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_batches: 3560, num_test_batches: 3561\n"
     ]
    }
   ],
   "source": [
    "num_train_batches = len(train_loader)\n",
    "num_test_batches = len(test_loader)\n",
    "\n",
    "print(f\"num_train_batches: {num_train_batches}, num_test_batches: {num_test_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9cc57a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_mnist = Net(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.SGD(net_mnist.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da740139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 100]          78,500\n",
      "            Linear-2                  [-1, 100]          10,100\n",
      "            Linear-3                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.34\n",
      "Estimated Total Size (MB): 0.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net_mnist, input_size=(INPUT_SIZE,), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b61cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_pytorch_minibatch(net, train_loader, optimizer, criterion, num_epochs):\n",
    "\n",
    "    net.train() \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9160ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_minibatch(net, data_loader):\n",
    "    \"\"\"\n",
    "    Calculates the overall accuracy by using minibatches\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = net(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)  \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    accuracy = correct/len(data_loader.dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b6c9d190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea7e9f0140847428f5edbde7766ed37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "117323",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 117323",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_neural_network_pytorch_minibatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_mnist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[66], line 6\u001b[0m, in \u001b[0;36mtrain_neural_network_pytorch_minibatch\u001b[0;34m(net, train_loader, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain() \n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader)):\n\u001b[1;32m      8\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m net(data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 117323"
     ]
    }
   ],
   "source": [
    "train_neural_network_pytorch_minibatch(net_mnist, train_loader, optimizer, criterion, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc4c7b27",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "078d3e7d4d77df211c92872e8e23661b",
     "grade": false,
     "grade_id": "cell-951632347ae286bd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def leader_board_predict_fn(values):\n",
    "    \n",
    "    decision_function_values = np.zeros(values.shape[0])\n",
    "\n",
    "     \n",
    "    model = joblib.load(\"./logistic_regression_model.pkl\")\n",
    "    return model.predict(values)\n",
    "    \n",
    "    return decision_function_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b350d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5b91dddd2a088ce77c347337e55b7ef",
     "grade": false,
     "grade_id": "cell-feda725da2aaae3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Testing your Implementation\n",
    "Your model should return the probability or decision function value that indicates the likelyhood of fraud for each input transaction. To verify that this is the case, we run your model on a subset of the transactions dataset it was trained on. There is a hidden cell that performs the actual test on the unseen test set and computes your score for the leaderboard using the [ROC AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f4bfc30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3300721a523cb82c46174de8ca5c41c",
     "grade": true,
     "grade_id": "cell-973aee3c6a885301",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "def get_score():\n",
    "    \"\"\"\n",
    "    Function to compute scores for train and test datasets.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import pathlib\n",
    "\n",
    "    try:\n",
    "        path = \"/data/mlproject22\" if os.path.exists(\"/data/mlproject22\") else \".\"\n",
    "        test_data = pd.read_csv(os.path.join(path, \"transactions.csv.zip\"))\n",
    "        X_test = test_data.drop(columns = \"Class\")\n",
    "        y_test = test_data[\"Class\"]\n",
    "        decision_function_values = leader_board_predict_fn(X_test)\n",
    "        assert decision_function_values.shape == (X_test.shape[0],)\n",
    "        dataset_score = roc_auc_score(y_test, decision_function_values)\n",
    "        assert dataset_score >= 0.0 and dataset_score <= 1.0\n",
    "    except Exception:\n",
    "        dataset_score = float(\"nan\")\n",
    "    print(f\"Train Dataset Score: {dataset_score}\")\n",
    "\n",
    "    import os\n",
    "    import pwd\n",
    "    import time\n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    user_id = pwd.getpwuid( os.getuid() ).pw_name\n",
    "    curtime = time.time()\n",
    "    dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    try:\n",
    "        HIDDEN_DATASET_PATH = os.path.expanduser(\"/data/mlproject22-test-data\")\n",
    "        test_data = pd.read_csv(os.path.join(HIDDEN_DATASET_PATH,\"transactions_scoreboard.csv.zip\"))\n",
    "        X_test = test_data.drop(columns=[\"Class\"])\n",
    "        y_test = test_data[\"Class\"]\n",
    "        decision_function_values = leader_board_predict_fn(X_test)\n",
    "        hiddendataset_score = roc_auc_score(y_test, decision_function_values)\n",
    "        print(f\"Test Dataset Score: {hiddendataset_score}\")\n",
    "        score_dict = dict(\n",
    "            score_hidden=hiddendataset_score,\n",
    "            score_train=dataset_score,\n",
    "            unixtime=curtime,\n",
    "            user=user_id,\n",
    "            dt=dt_now,\n",
    "            comment=\"\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        err = str(e)\n",
    "        score_dict = dict(\n",
    "            score_hidden=float(\"nan\"),\n",
    "            score_train=dataset_score,\n",
    "            unixtime=curtime,\n",
    "            user=user_id,\n",
    "            dt=dt_now,\n",
    "            comment=err\n",
    "        )\n",
    "\n",
    "    if list(pathlib.Path(os.getcwd()).parents)[0].name == 'source':\n",
    "        print(\"we are in the source directory... replacing values.\")\n",
    "        print(pd.DataFrame([score_dict]))\n",
    "        score_dict[\"score_hidden\"] = -1\n",
    "        score_dict[\"score_train\"] = -1\n",
    "        print(\"new values:\")\n",
    "        print(pd.DataFrame([score_dict]))\n",
    "\n",
    "    pd.DataFrame([score_dict]).to_csv(\"transactions.csv\", index=False)\n",
    "    \n",
    "get_score()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "352px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
